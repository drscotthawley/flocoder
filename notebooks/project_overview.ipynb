{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9c71745-faf4-41ea-a179-999c83896242",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Preamble: Slides via RISE\n",
    "\n",
    "This notebook is a set of slides made using `jupyterlab-rise`, installed via   \n",
    " \n",
    "```bash\n",
    "$ uv pip install jupyterlab_rise\n",
    "```\n",
    "\n",
    "When opened in `jupyterlab`, press`Ctrl+R` (or `Option+R` on Mac) to render\n",
    "\n",
    "P.S.- Should have just used Google Slides. Spent way too much time mucking about with CSS!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c5d7eb-17dd-4194-8169-3fa15d278283",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# flocoder: Project Overview / Recruitment Drive \n",
    "\n",
    "### Scott H. Hawley \n",
    "- Professor of Physics, Belmont University, Nashville TN USA\n",
    "- Head of Research, Hyperstate Music AI\n",
    "- Former Technical Fellow, Stability AI (co-author on Stable Audio paper)\n",
    "\n",
    "ICLR 2025 \"Best Blog Post\": \"**Flow With What You Know**: basic physics provides a 'straight, fast' way to get up to speed on flow-based generative models\"\n",
    "\n",
    "<center>\n",
    "<a href=\"https://iclr.cc/media/PosterPDFs/ICLR%202025/31364.png?t=1745186162.1069727\"><img src=\"https://iclr.cc/media/PosterPDFs/ICLR%202025/31364.png?t=1745186162.1069727\" width=\"40%\"></a></center>\n",
    "\n",
    "*Apr 23, 2025, SUTD*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ce131a-3c42-4ab8-b843-87fae3019f65",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# (My) Prior Work: Experimenting with Controllable MIDI Generation\n",
    "\n",
    "My experience with LLM-like Transformer-based MIDI gen models was unimpressive:\n",
    "\n",
    "* compositions didn't \"sound great,\"\n",
    "* quickly devolved,\n",
    "* even with lots of work & tweaking.\n",
    "* Limited/no options for controlling outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6333ee65-fcbd-4c9a-9789-4b6e0b171ecb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# (My) Prior Work: Pictures of MIDI\n",
    "\n",
    "Idea: GUI for controllable gen: users draw an inpainting mask of *roughly* where they want notes to go. Have the system generate notes that fit. (Not a new idea, just new-to-me)\n",
    "\n",
    "[\"Pictures of MIDI\"](https://arxiv.org/abs/2407.01499) https://arxiv.org/abs/2407.01499: \n",
    "<center>\n",
    "  <img src=\"pom_mot_idea.png\" width=\"35%\" style=\"margin: 0; padding: 0;\">\n",
    "</center>\n",
    "\n",
    "Operate on piano-roll *image representations* of MIDI. Based on [â€œPolyffusion\" (Min et al, 2023)](https://arxiv.org/abs/2307.10304), using code from [HDiT (Crowson et al, 2024)](https://arxiv.org/abs/2401.11605) \n",
    "\n",
    "Worked great!<sup>1</sup> *Amazingly easy* to do: melody, accompaniment, extension,... \n",
    "* HF Spaces Demo: https://huggingface.co/spaces/drscotthawley/PicturesOfMIDI\n",
    "* Demo page: https://picturesofmidi.github.io/PicturesOfMIDI/\n",
    " \n",
    "\n",
    "\n",
    "<div class=\"footnote\"><sup>1</sup>...But big and slow</div>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986efad5-ef22-49a3-be88-749e726c929e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Idea: Small (Interpretable?) Latent Flow Model\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/drscotthawley/flocoder/refs/heads/main/images/flow_schematic.jpg\" width=\"30%\"><br>\n",
    "</center>\n",
    "\n",
    "Piano roll (PR) images are mostly empty space. So compress to some small latent space. \n",
    "* Pretrained VQGAN/VQVAE<sup>1</sup>  from Stable Diffusion yielded \"janky\" results. Probably needlessly \"general\".\n",
    "* So train a custom VQGAN for MIDI PR's to get good compression.\n",
    "\n",
    "Wish list: Get VQ latents to compress via repeated musical phrases, use encoder for *Motif Analysis*? (Not tried yet)\n",
    "\n",
    "<div class=\"footnote\"><sup>1</sup> Terminology: VQGAN is a VQVAE that has self-attention and is trained with adversarial loss.</div>   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4a612f-985d-4123-a813-93eb352777ce",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# MIDI-VQGAN\n",
    "\n",
    "Takes 128x128x3 RGB images, compresses them to 16x16x4: 4 codebooks of residual vector quantization (RVQ), 32 vectors per codebook\n",
    "\n",
    "Would like better compression, this just worked for now. \n",
    "\n",
    "98% reconstruction accuracy (F1 score) using POP909 dataset. (Tan line:)\n",
    "\n",
    "<center>\n",
    "<img src=\"https://cdn.discordapp.com/attachments/1336763902175744143/1338943564528357387/472747754_453113941185849_3823178967569815371_n.png?ex=680934de&is=6807e35e&hm=b773b727e8fa17189c34de26d39dc0d1b2cd5992626fad920fe0aaaa73e61d07&\" width=\"40%\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fa4161-34a8-4f5f-aa07-656d280500be",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Flow Model *Seems* Good, Decoding seems *Meh* \n",
    "\n",
    "Pretty simple. Works ok, but the problem is that the decoded outputs look a bit \"off\": horizontal-only green lines get \"steps\"\n",
    "\n",
    "<center>\n",
    "  <div style=\"width: 70%; overflow: hidden; display: inline-block; position: relative;\">\n",
    "    <div style=\"width: 300%; margin-left: -100%; position: relative;\">\n",
    "      <img src=\"https://cdn.discordapp.com/attachments/1336763902175744143/1339058461274800230/image.png?ex=6808f71f&is=6807a59f&hm=1f6911aae81558fa2199e0844d8a92615029cfa180d0b942ca2bb613600b96dd&\" \n",
    "           style=\"width: 100%; display: block;\">\n",
    "    </div>\n",
    "    <div style=\"position: absolute; top: 10px; left: 0; right: 0; text-align: center; color: white; font-weight: bold; font-size:2em; text-shadow: 1px 1px 3px black;\">Original</div>\n",
    "    <div style=\"position: absolute; bottom: 10px; left: 0; right: 0; text-align: center; color: white; font-weight: bold; font-size:2em; text-shadow: 1px 1px 3px black;\">Generated</div>\n",
    "  </div>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ba91a6-ba82-4bff-ad0e-22365b908377",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Distribution of Flow Gen vs. Original\n",
    "\n",
    "Despite slight \"mangling\", distributions look very close\n",
    "\n",
    "<center><img src=\"https://cdn.discordapp.com/attachments/1336763902175744143/1339037434498912286/image.png?ex=6808e38a&is=6807920a&hm=2d6dea3c256ab3ce8c19fb1f9462c24805c5298af9357069725372b354b2a6b7&\" width=\"40%\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf65ee73-6d25-4069-900e-ed49a3d0b587",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Q: Why *Decode* to Images at All?\n",
    "\n",
    "* If the goal is to generate MIDI, then why bother decoding to images? \n",
    "* Why not just decode direclty to MIDI? \n",
    "\n",
    "*A: because I was just porting from a working image-diffusion code to custom vq-flow, and didn't think of it til recently*\n",
    "\n",
    "Current workflow seems maybe *absurdly* wasteful:\n",
    "\n",
    "1. Get MIDI\n",
    "2. Convert to PR Image\n",
    "3. User draws mmask\n",
    "4. Encode\n",
    "5. Flow\n",
    "6. Quantize\n",
    "7. Decode to PR Image (slower than I'd like)\n",
    "8. Convert to MIDI\n",
    "\n",
    "only did it because the MIDI-image-inpainting method was so powerful and easy!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd434a66-279c-48b9-82d7-17021d49c3cf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Open Q: How to Use Latents for Motifs?\n",
    "\n",
    "Two interests (possibly opposing): \n",
    "\n",
    "## I.  Interpretable Representation?\n",
    "\n",
    "* MIDI *itself* is the compressed, interpretable representation\n",
    "* Maybe some kind of CLIP-like contrastive loss / VICReg mapping between latents and (projected) MIDI?\n",
    "\n",
    "## 2. Compressing via Repeated Motifs? \n",
    "* BPE tends to multiply too many tokens\n",
    "* (factorizable) n-grams are ok\n",
    "* kernel-grams>\n",
    "\n",
    "\n",
    "\n",
    "...Any ideas? (Save for Discussion at end)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5070670d-89d6-4a60-9aa5-9cf6bc85c9d1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Issue: Education vs. Performance\n",
    "\n",
    "This also started as a *teaching* project, writing all code from scratch so \"we\" understand it all\n",
    "\n",
    "But maybe best to repurpose others' codes (e.g. SD, Meta,...) for performance? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c75d656-0729-4d88-9d68-1aff145fb753",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Thanks, Collab & Discussion\n",
    "Thanks to you! And to Raymond Fan for helpful discussions. \n",
    "\n",
    "**Collaborators?** This *has been* largely a solo project but would *love* to make it a collaborative effort! Glady share credits, can MIT license code,...  Grad students? ;-)\n",
    "\n",
    "Discussion..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
